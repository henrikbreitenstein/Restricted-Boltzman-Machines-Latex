@online{LipkinQuasiToPauli,
  author = {Morten Hjort-Jensen},
  title = {Quantum computing and solving the eigenvalue problem for the Lipkin model},
  year = {2023},
  url = {https://github.com/CompPhysics/QuantumComputingMachineLearning/blob/gh-pages/doc/pub/week9/ipynb/week9.ipynb},
  urldate = {2023-11-10}
}
@online{LipkinRewrite,
  author = {Morten Hjort-Jensen},
  title = {Quantum computing and solving the eigenvalue problem for the Lipkin model},
  year = {2024},
  url = {https://github.com/CompPhysics/QuantumComputingMachineLearning/blob/gh-pages/doc/pub/week8/ipynb/week8.ipynb},
  urldate = {2024-04-17}
}
@article{LIPKIN1965188,
    author = {H.J. Lipkin and N. Meshkov and A.J. Glick},
    title = {Validity of many-body approximation methods for a solvable model: (I). Exact solutions and perturbation theory},
    journal = {Nuclear Physics},
    volume = {62},
    number = {2},
    pages = {188-198},
    year = {1965},
    issn = {0029-5582},
    doi = {https://doi.org/10.1016/0029-5582(65)90862-X},
    url = {https://www.sciencedirect.com/science/article/pii/002955826590862X},
    abstract = {In order to test the validity of various techniques and formalisms developed for treating many-particle systems, a model is constructed which is simple enough to be solved exactly in some cases, but yet is non-trivial. The construction of such models is based on the observation that bilinear products of creation and annihilation operators can be considered as generators of Lie groups. Thus the problem of finding eigenvalues can be greatly simplified by the additional integrals of the motion which are present if the Hamiltonian is constructed so as to commute with invariants of the group. In the present case, the model consists of N fermions distributed in two N-fold degenerate levels and interacting via a monopole-monopole force. It is shown that the model Hamiltonian is easily expressed in terms of quasi-spin operators and exact eigenvalues are obtained. In addition, eigenvalues are calculated with ordinary perturbation theory using values for the number of particles and interaction strength which are appropriate to the more realistic problems of finite nuclei. In subsequent papers we consider the results obtained by various other approximation methods for comparison with the exact results presented here.}
}

@book{RosenblattBack,
    author={Rosenblatt, Frank.},
    title={Principles of neurodynamics; perceptrons and the theory of brain mechanisms.},
    year={1962},
    publisher={Spartan Books},
    address={Washington},
    pages={616 p.},
    keywords={Perceptrons; Brain / Mathematical models.},
    note={Bibliography : p. 609-616},
    url={http://hdl.handle.net/2027/mdp.39015039846566}
}

@article{Linnainmaa1976,
    author={Linnainmaa, Seppo},
    title={Taylor expansion of the accumulated rounding error},
    journal={BIT Numerical Mathematics},
    year={1976},
    month={Jun},
    volume={16},
    number={2},
    pages={146-160},
    abstract={The article describes analytic and algorithmic methods for determining the coefficients of the Taylor expansion of an accumulated rounding error with respect to the local rounding errors, and hence determining the influence of the local errors on the accumulated error. Second and higher order coefficients are also discussed, and some possible methods of reducing the extensive storage requirements are analyzed.},
    issn={1572-9125},
    doi={10.1007/BF01931367},
    url={https://doi.org/10.1007/BF01931367}
}


@article{Rumelhart1986,
    author={Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
    title={Learning representations by back-propagating errors},
    journal={Nature},
    year={1986},
    month={Oct}, 
    volume={323},
    number={6088},
    pages={533-536},
    abstract={We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure.},
    issn={1476-4687},
    doi={10.1038/323533a0},
    url={https://doi.org/10.1038/323533a0}
}
@article{2020SciPy-NMeth,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  url  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2}
}
@article{ancoopcomp,
    author = {G. E. Hinton and T. J. Sejnowski},
    title = {Analyzing Cooperative Computation},
    journal = {Fifth annual connference of the Cogniitive Science Society},
    year = {1983}
}

@article{boltzderv,
    author = {A.L. Yuille},
    title = {Boltzmann Machine},
    journal = {JHU},
    year = {2016}
}

@misc{NVIDIA,
  author={NVIDIA},
  title={NVIDIA},
  year={2024},
  url={https://www.nvidia.com/en-us/about-nvidia/#About%20Us}
}
@article{paul,
author = {Smolensky, Paul},
year = {1986},
month = {01},
pages = {194-281},
title = {Information processing in dynamical systems: Foundations of harmony theory},
volume = {1},
journal = {Parallel Distributed Process}
}

@article{PhysRevLett.108.058301,
  title = {Fast and Accurate Modeling of Molecular Atomization Energies with Machine Learning},
  author = {Rupp, Matthias and Tkatchenko, Alexandre and M\"uller, Klaus-Robert and von Lilienfeld, O. Anatole},
  journal = {Phys. Rev. Lett.},
  volume = {108},
  issue = {5},
  pages = {058301},
  year = {2012},
  month = {Jan},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.108.058301},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.108.058301}
}
@article{Carleo_2017,
    author = {Giuseppe Carleo  and Matthias Troyer },
    title = {Solving the quantum many-body problem with artificial neural networks},
    journal = {Science},
    volume = {355},
    number = {6325},
    pages = {602-606},
    year = {2017},
    doi = {10.1126/science.aag2302},
    url = {https://www.science.org/doi/abs/10.1126/science.aag2302},
    eprint = {https://www.science.org/doi/pdf/10.1126/science.aag2302},
    abstract = {Elucidating the behavior of quantum interacting systems of many particles remains one of the biggest challenges in physics. Traditional numerical methods often work well, but some of the most interesting problems leave them stumped. Carleo and Troyer harnessed the power of machine learning to develop a variational approach to the quantum many-body problem (see the Perspective by Hush). The method performed at least as well as state-of-the-art approaches, setting a benchmark for a prototypical two-dimensional problem. With further development, it may well prove a valuable piece in the quantum toolbox. Science, this issue p. 602; see also p. 580 A machine-learning approach sets a computational benchmark for a prototypical two-dimensional problem. The challenge posed by the many-body problem in quantum physics originates from the difficulty of describing the nontrivial correlations encoded in the exponential complexity of the many-body wave function. Here we demonstrate that systematic machine learning of the wave function can reduce this complexity to a tractable computational form for some notable cases of physical interest. We introduce a variational representation of quantum states based on artificial neural networks with a variable number of hidden neurons. A reinforcement-learning scheme we demonstrate is capable of both finding the ground state and describing the unitary time evolution of complex interacting quantum systems. Our approach achieves high accuracy in describing prototypical interacting spins models in one and two dimensions.}
}

@misc{cuda,
  author={NVIDIA and Vingelmann, P\'{e}ter and Fitzek, Frank H.P.},
  title={Cuda compilation tools, release 12.3, V12.3.103},
  year={2023},
  url={https://developer.nvidia.com/cuda-toolkit},
}

@misc{paszke2019pytorch,
      title={PyTorch: An Imperative Style, High-Performance Deep Learning Library}, 
      author={Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas K\"{o}pf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
      year={2019},
      eprint={1912.01703},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@online{DerivationGradient,
      title={Derivation of gradient of the expectation of local energy}, 
      author={Unknown author},
      year={2019},
      url={https://physics.stackexchange.com/questions/473533/derivation-of-gradient-of-the-expectation-of-local-energy},
      urldate={2024-02-09}
}

@book{Goodfellow,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}
@book{Robert1999,
    author={Robert, Christian P. and Casella, George},
    title={The Gibbs Sampler},
    bookTitle={Monte Carlo Statistical Methods},
    year={1999},
    publisher={Springer New York},
    address={New York, NY},
    pages={285-361},
    abstract={The previous chapter developed simulation techniques that could be called generic, since they require only a limited amount of information about the distribution to be simulated. For example, the generic algorithm ARMS (6.3.3) aims at reproducing the density f of this distribution in an automatic manner. However, Metropolis-Hastings algorithms can achieve higher levels of efficiency when they take into account the specifics of the target distribution f, in particular through the calibration of the acceptance rate (see 6.4.1). Moving even further in this direction, the properties and performance of the Gibbs sampling method presented in this chapter are very closely tied to the distribution f. This is because the choice of instrumental distribution is essentially reduced to a choice between a finite number of possibilities.},
    isbn={978-1-4757-3071-5},
    doi={10.1007/978-1-4757-3071-5_7},
    url={https://doi.org/10.1007/978-1-4757-3071-5_7}
}

@article{netket3:2022,
	title={NetKet 3: Machine Learning Toolbox for Many-Body Quantum Systems},
  author={Filippo Vicentini and Damian Hofmann and Attila Szab\'{o} and Dian Wu and Christopher Roth and Clemens Giuliani and Gabriel Pescia and Jannes Nys and Vladimir Vargas-Calderón and Nikita Astrakhantsev and Giuseppe Carleo},
	journal={SciPost Phys. Codebases},
	pages={7},
	year={2022},
	publisher={SciPost},
	doi={10.21468/SciPostPhysCodeb.7},
	url={https://scipost.org/10.21468/SciPostPhysCodeb.7}
}

@article{harris2020array,
 title={Array programming with NumPy},
 author={Charles R. Harris and K. Jarrod Millman and St{\'e}fan J. van der Walt and Ralf Gommers and Pauli Virtanen and David Cournapeau and Eric Wieser and Julian Taylor and Sebastian Berg and Nathaniel J. Smith and Robert Kern and Matti Picus and Stephan Hoyer and Marten H. van Kerkwijk and Matthew Brett and Allan Haldane and Jaime Fern{\'a}ndez del R{\'i}o and Mark Wiebe and Pearu Peterson and Pierre G{\'e}rard-Marchant and Kevin Sheppard and Tyler Reddy and Warren Weckesser and Hameer Abbasi and Christoph Gohlke and Travis E. Oliphant},
 year={2020},
 month={Sep},
 journal={Nature},
 volume={585},
 number={7825},
 pages={357-362},
 doi={10.1038/s41586-020-2649-2},
 publisher={Springer Science and Business Media LLC},
 url={https://doi.org/10.1038/s41586-020-2649-2}
}


@article{netket2:2019,
  author    = {Carleo, Giuseppe and Choo, Kenny and Hofmann, Damian and
      Smith, James E.T. and Westerhout, Tom and Alet, Fabien and
      Davis, Emily J. and Efthymiou, Stavros and Glasser, Ivan and
      Lin, Sheng-Hsuan and Mauri, Marta and Mazzola, Guglielmo and
      Mendl, Christian B. and van Nieuwenburg, Evert and
      O'Reilly, Ossian and Th{\'e}veniaut, Hugo and Torlai, Giacomo and Vicentini, Filippo and
      Wietek, Alexander},
  title     = {NetKet: A Machine Learning Toolkit for Many-Body Quantum Systems},
  journal   = {SoftwareX},
  pages     = {100311},
  doi       = {10.1016/j.softx.2019.100311},
  url       = {http://www.sciencedirect.com/science/article/pii/S2352711019300974},
  year      = {2019}
}

@article{mpi4jax:2021,
    doi = {10.21105/joss.03419},
    url = {https://doi.org/10.21105/joss.03419},
    year = {2021},
    publisher = {The Open Journal},
    volume = {6},
    number = {65},
    pages = {3419},
    author = {Dion H\"{a}fner and Filippo Vicentini},
    title = {mpi4jax: Zero-copy MPI communication of JAX arrays},
    journal = {Journal of Open Source Software}
  }

@online{netketising,
  author = {Filippo Vicentini (EPFL-CQSL)},
  title = {Ground-State: Ising model},
  year = {2024},
  url = {https://netket.readthedocs.io/en/v3.11.4/tutorials/gs-ising.html},
  urldate = {2024-04-17}
}


@article{gehinton,
author = {G. E. Hinton and R. R. Salakhutdinov},
title = {Reducing the Dimensionality of Data with Neural Networks},
journal = {Science},
volume = {313},
number = {5786},
pages = {504-507},
year = {2006},
doi = {10.1126/science.1127647},
URL = {https://www.science.org/doi/abs/10.1126/science.1127647},
eprint = {https://www.science.org/doi/pdf/10.1126/science.1127647},
abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.}
}

@article{Xia2018,
author={Xia, Rongxin
and Kais, Sabre},
title={Quantum machine learning for electronic structure calculations},
journal={Nature Communications},
year={2018},
month={Oct},
volume={9},
number={1},
pages={4195},
abstract={Considering recent advancements and successes in the development of efficient quantum algorithms for electronic structure calculations---alongside impressive results using machine learning techniques for computation---hybridizing quantum computing with machine learning for the intent of performing electronic structure calculations is a natural progression. Here we report a hybrid quantum algorithm employing a restricted Boltzmann machine to obtain accurate molecular potential energy surfaces. By exploiting a quantum algorithm to help optimize the underlying objective function, we obtained an efficient procedure for the calculation of the electronic ground state energy for a small molecule system. Our approach achieves high accuracy for the ground state energy for H2, LiH, H2O at a specific location on its potential energy surface with a finite basis set. With the future availability of larger-scale quantum computers, quantum machine learning techniques are set to become powerful tools to obtain accurate values for electronic structures.},
issn={2041-1723},
doi={10.1038/s41467-018-06598-z},
url={https://doi.org/10.1038/s41467-018-06598-z}
}

@article{vanNieuwenburg2017,
author={van Nieuwenburg, Evert P. L. and Liu, Ye-Hua and Huber, Sebastian D.},
title={Learning phase transitions by confusion},
journal={Nature Physics},
year={2017},
month={May},
volume={13},
number={5},
pages={435-439},
abstract={A neural-network technique can exploit the power of machine learning to mine the exponentially large data sets characterizing the state space of condensed-matter systems. Topological transitions and many-body localization are first on the list.},
issn={1745-2481},
doi={10.1038/nphys4037},
url={https://doi.org/10.1038/nphys4037}
}


@article{PhysRevApplied.14.014011,
  title = {Single-Exposure Absorption Imaging of Ultracold Atoms Using Deep Learning},
  author = {Ness, Gal and Vainbaum, Anastasiya and Shkedrov, Constantine and Florshaim, Yanay and Sagi, Yoav},
  journal = {Phys. Rev. Appl.},
  volume = {14},
  issue = {1},
  pages = {014011},
  year = {2020},
  month = {Jul},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevApplied.14.014011},
  url = {https://link.aps.org/doi/10.1103/PhysRevApplied.14.014011}
}

@article{RAISSI2019686,
title = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
journal = {Journal of Computational Physics},
volume = {378},
pages = {686-707},
year = {2019},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2018.10.045},
url = {https://www.sciencedirect.com/science/article/pii/S0021999118307125},
author = {M. Raissi and P. Perdikaris and G.E. Karniadakis},
keywords = {Data-driven scientific computing, Machine learning, Predictive modeling, Runge–Kutta methods, Nonlinear dynamics},
abstract = {We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.}
}

