\section{Calculation of the local energy}

We know that the local energy of a particular sample state $\ket{s}$ can be calculated as

\begin{equation}
  E_{loc} = \displaystyle\frac{\bra{s}H\ket{\psi_{rbm}}}{\braket{s}{\psi_{rbm}}} \; ,
  \label{eq:local_imp}
\end{equation}

but here we will explain in more detail how this is done for the each of the different systems we are looking at. First of all it is important to remember the structure of the input to our function calculating the local energy. We want to vectorize the calculations as much as possible so the input will be all our samples, taken with the Gibbs or Metropolis-Hastings algorithm, together in one array:

\begin{equation}
  \mathbf{S} = \left [ s_0, s_1, \dots, s_n \right] \; .
  \label{eq:Samples_set}
\end{equation}

Our operations will then be done on all the samples at once, decreasing computation time by vectorization as well as opening up for use of the GPU. To use the definition of the local energy, \ref{eq:local_imp}, we need to extract the machine state from our set of samples. As described in \ref{sec:nnqs} we check the distribution by extracting all unique states in our set of samples and setting the amplitudes as the square root of their relative occurrence in the set, presuming that all are positive definite.

\begin{minted}{python}
  
def unique_states_amplitude(samples):
    size = samples.shape[0]
    unique, weight = torch.unique(samples, dim=0, return_counts=True)
    weight = torch.sqrt(weight/size)

    return unique, weight
\end{minted}

We then have a approximation of our machine state $\ket{\psi}$. The next step of using each state in the sample set is varies with the hamiltonian of the system.

\subsection{The Lipkin model}

The Lipkin model hamiltonian is defined as

\begin{equation} 
    H = \sum_{p\sigma} \left ( \frac{1}{2} \sigma \varepsilon \right ) \hc{a}{p\sigma}\op{a}{p\sigma} + \frac{V}{2}\sum_{pp'\sigma} \hc{a}{p\sigma} \hc{a}{p'\sigma} \op{a}{p'-\sigma}\op{a}{p-\sigma} + \frac{W}{2} \sum_{pp'\sigma} \hc{a}{p\sigma} \hc{a}{p'-\sigma} \op{a}{p'\sigma}\op{a}{p-\sigma} \; , 
\end{equation}

For a more structured explanation we separate the hamiltonian in three parts:

\begin{align}
  H_{\varepsilon} &= \sum_{p\sigma} \left ( \frac{1}{2} \sigma \varepsilon \right ) \hc{a}{p\sigma}\op{a}{p\sigma}\\ 
  H_V &= \frac{V}{2}\sum_{pp'\sigma} \hc{a}{p\sigma} \hc{a}{p'\sigma} \op{a}{p'-\sigma}\op{a}{p-\sigma}\\
  H_W &= \frac{W}{2} \sum_{pp'\sigma} \hc{a}{p\sigma} \hc{a}{p'-\sigma} \op{a}{p'\sigma}\op{a}{p-\sigma}  \; .
  \label{eq:LMG_split_H}
\end{align}

 A state $\ket{b}$ contributes for the different parts as follows:

\begin{itemize}
  \item $H_{\varepsilon}$ - the same state $\ket{b}$.
  \item $H_V$ - states that are a excited or deexcited pair away from the state $\ket{b}$.
  \item $H_W$ - states one excited or deexcited particle away from $\ket{b}$.
\end{itemize}

For the $H_{\varepsilon}$ contribution we start of by calculating the number of particles in the first and second layer.

\begin{minted}{python}
    N_0 = torch.sum(unique == 0, dim=-1)
    N_1 = torch.sum(unique == 1, dim=-1)
\end{minted}

The particles in the first layer contribute with negative spin and the particles in the second layer contribute positivly, so we get that.

\begin{minted}{python} 
    H_0 = 0.5*eps*(N_1-N_0)
\end{minted}

We then have an array of the machine states amplitudes multiplied by $\varepsilon$. Applying \mintinline{python}{mask} to this array then selects the correct value for each sample without having to calculate duplicates more than once. We then have the $H_{\varepsilon}$ contribution for each sample directly:

\begin{minted}{python} 
    H_eps = H_0[mask]
\end{minted}

We then calculate the difference between the basis states of the machine state, which is the states in \mintinline{python}{unique}, and the number of particles that change layer.

\begin{minted}{python}
    diff_unique = abs(torch.sum(unique[:, None] - unique), dim=-1)
    diff_N1 = abs(N_1[:, None] - N_1)
\end{minted}

The vector arrays are being broadcast to a $N\times N$ matrix. If we denote \mintinline{python}{unique} $ = \boldsymbol{u}$ we have that \mintinline{python}{unique[:, None] - unique} becomes

\begin{equation}
  \boldsymbol{u}^T - \boldsymbol{ u } = \begin{bmatrix}
    u_0 \\ u_1 \\ \vdots \\ u_n
    \end{bmatrix} - \begin{bmatrix} u_0 & u_1 & \dots & u_n \end{bmatrix} \rightarrow \begin{bmatrix}
    u_0-u_0& u_0-u_1& \dots& u_0-u_n \\
    u_1-u_0& u_1-u_1& \dots& u_1-u_n \\
           & & \vdots & & \\
    u_n-u_0&u_n-u_1 & \dots & u_n-u_n

  \end{bmatrix}
  \label{eq:imp_ham_lip_unique}
\end{equation}

Taking the absolute value of the sum of these $u_i - u_j$ elements we get the total number of particles that either excite or de-excite from one state to the next. For the $H_V$ contributions we first need to find each possible one-pair-different basis states. The \mintinline{python}{diff_unique} array does not confirm that a two-particles difference comes the same layer, however, so we need to check with the change in particles in one of the layers as well. We calculate the $H_V$ contribution for the basis states as follows:

\begin{minted}{python} 
    one_pair = np.bitwise_and(diff_unique==2, diff_N1==2)
    H_1 = V*torch.sum(weight[:, None]*one_pair, dim=0)/weight
\end{minted}

where the \mintinline{python}{np.bitwise_and} outputs a $1$ only if both inputs are $1$. 

We check each of the unique states, $\ket{b_i}$, with all the other unique states, $\ket{b_j}$, and see if there are a one-pair-difference between them, setting the element $\left (i, j \right )$ to $1$ if that is the case. As a result we end up with a $N\times N$ matrix, $\mathbf{V}$, where $N$ is the number of unique states. Each pair represents the element
\begin{equation}
  \boldsymbol{V}_{i,j} =\frac{\bra{b_i}H_V\ket{b_j}}{\braket{b_i}{\psi_{rbm}}} = \frac{V\alpha_j}{\alpha_i} \; ,
  \label{eq:LMG_imp_V_cont}
\end{equation}

where $\alpha_i$ and $\alpha_j$ is respectively the amplitude of the basis state $\ket{b_i}$ and $\ket{b_j}$ in the machine state $\psi_{rbm}$. This means we need to scale our matrix $\mathbf{V}$ with the interaction strength $V$. Then scale each row $i$ with $\frac{1}{\alpha_i}$ and each column $j$ with the corresponding states amplitude $\alpha_j$. Each row then represents the elements of 

$$ E_{loc, V}(\ket{b_i}) = \frac{\bra{b_i}H_V\ket{\psi_{rbm}}}{\braket{b_i}{\psi_{rbm}}} \; , $$

and since the amplitudes are already assumed to be positive definite, we can then calculate the total contribution of the basis state $\ket{b_i}$:

\begin{equation}
 E_{loc, V}(\ket{b_i})= \sum_j \mathbf{V}_{i,j} \; ,
\end{equation}
which we can then apply \mintinline{python}{mask} to and get the $H_V$ contribution for each sample:

\begin{minted}{python}
    H_1 = V*torch.sum(
        weight[:, None]*(
            abs(torch.sum((unique[:, None] - unique), dim=-1)) == 2), dim=0)/weight
    H_V = H_1[mask]
\end{minted}

For $H_W$ we similarly find the unique states that are one particle different from each other. Scaling it with the interaction strength $W$, the rows and columns with the amplitude of the respective basis state and sum along the second axis:

\begin{minted}{python} 
    H_2 = W*torch.sum(
        weight[:, None]*(
            abs(torch.sum((unique[:, None] - unique), dim=-1)) == 1) , dim=0)/weight
    H_W = H_2[mask]
\end{minted}

The total hamiltonian we have as

\[
  H = H_{\varepsilon} + H_V + H_W \; ,
\]

which we can calculate directly:

\begin{minted}{python} 
    E = (H_eps + H_V + H_W)
    return E
\end{minted}


\subsection{The Ising model}

For the Ising model we can calculate the local energy directly with:

\begin{equation}
  H(\boldsymbol{\sigma}) = J\sum_{<i,j>}\sigma_i\sigma_j \; ,
  \label{eq:imp_hamil_ising}
\end{equation}

where $J$ is the interaction strength between nearest neighbors. To calculate this efficiently we take the whole set and shift it:

\begin{minted}{python}
  def ising_local(samples):
    spin = 2*samples - 1
    shifted = torch.roll(spin, dim=1)
\end{minted}

where we then assume the boundary conditions:

\begin{equation}
  \mathbf{\sigma}_0 = \mathbf{\sigma}_N \; ,
  \label{eq:ising_boundary_imp}
\end{equation}

as explained in \ref{sec:ising_theory}. We can then easily calculate the energy of the configuration with a sum:

\begin{minted}{python}
    H_J = J*torch.sum(shifted*spin, dim=1)
    H_L = L*torch.sum(spin, dim=1)
    return H_J + H_L
\end{minted}

where $H_J$ is the iarenteraction part and $H_L$ is the external field part.

\subsection{The Pairing model}

The Pairing model hamiltonian is defined as:

\begin{equation}
  H = \dots
  \label{eq:Pairing_model_imp}
\end{equation}

We then start with

\subsection{The Heisenberg model}

The Heisenberg model hamiltonian we have as:

\begin{equation}
  H = \dots
  \label{eq:Heisenberg_model_imp}
\end{equation}


