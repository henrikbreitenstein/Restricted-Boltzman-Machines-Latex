\section{Calculation of the local energy}

At the sampling part we skipped over an important line.

\begin{minted}{python}
  E_local = hamiltonian.local_energy(dist_s)
\end{minted}

Where we have to calculate the local energy of the quantum system in question. We know that the local energy of a particular sample state $\ket{s}$ can be calculated as

\begin{equation}
  E_{loc} = \displaystyle\frac{\bra{s}H\ket{\psi_{rbm}}}{\braket{s}{\psi_{rbm}}} \; ,
  \label{eq:local_imp}
\end{equation}

but here we will explain in more detail how this is done for the each of the different systems we are looking at. First of all it is important to remember the structure of the input to our function calculating the local energy. We want to vectorize the calculations as much as possible so the input will be all $m$ samples, taken with the Gibbs or Metropolis-Hastings algorithm, together in one array:

\begin{equation}
  \mathbf{S} = \left [ s_0, s_1, \dots, s_m \right] \; ,
  \label{eq:Samples_set}
\end{equation}

where $s_0, s_1, \dots, s_n \in \mathbf{B}$, where $\mathbf{B}$ is the basis of the system. To calculate the local energy the wave function $\ket{\psi__{rbm}}$:

\begin{equation}
  \ket{\Psi_{rbm}} = \alpha_0\ket{\psi_0} + \alpha_1\ket{\psi_1} + \dots + \alpha_n\ket{\psi_n} \; ,
\end{equation}

but we don't have the exact coefficients $\alpha_0, \alpha_1, \dots, \alpha_n$, so we will need to approximate it. When we insert a random state into the hidden layer of the RBM and sample the it we can think of it as a measurement of the quantum state. If we denote $\ket{b}$ as the resulting state of such a measurement, the probability of a measurement resulting in a peculiar basis state is given by:

\begin{equation}
  P(\ket{b} = \ket{\psi_i}) =\right | \alpha_i \left |^2 \; .
  \label{eq:measurement_imp}
\end{equation}

And we can thus use our sample set $\mathbf{S}$, which is a collection of many measurements of the machine state, to approximate the coefficients $\{\alpha\}$. If we denote $N_i$ as the number of $\ket{\psi_i}$ we have in our sample set, we can approximate the machine state as:

\begin{equation}
  \ket{\Psi_{rbm}} \approx \sqrt{\frac{N_0}{m}}\ket{\psi_0} +\sqrt{\frac{N_1}{m}}\ket{\psi_1} +\dots + \sqrt{\frac{N_n}{m}}\ket{\psi_n} \; .
\end{equation}

If take a look at the different parts of the local energy, \ref{eq:local_imp}, we have the normalization factor for a basis state $\ket{b} = \ket{\psi_i} \in \mathbf{B}$.

$$\braket{b}{\Psi_{rbm}} = \alpha_i \; .$$


To calculate the numerator of $E_{local}$ we start with

\begin{equation}
  H\ket{\Psi_{rbm}} =  \alpha_0H\ket{\psi_0} + \alpha_1H\ket{\psi_1} + \dots + \alpha_nH\ket{\psi_n}
\end{equation}

The hamiltonian transforms the states into a new one

\begin{equation}
  H\ket{\psi_i} = \beta_{i,0}\ket{\psi_0} + \dots + \beta_{i,n}\ket{\psi_n}
\end{equation}

so we get

\begin{align}
  H\ket{\Psi_{rbm}} = \alpha_0  ( \beta_{0,0}\ket{\psi_0} &+ \dots + \beta_{0,n}\ket{\psi_n}  ) \\
 + \alpha_1  ( \beta_{1,0}\ket{\psi_0} &+ \dots + \beta_{1,n}\ket{\psi_n}  ) \\
&\vdots \\
+ \alpha_n  ( \beta_{n,0}\ket{\psi_0} &+ \dots + \beta_{n,n}\ket{\psi_n}  ) 
\end{align}

We then have that for the example state $\ket{b}$

\begin{equation}
  \bra{b}H\ket{\Psi_{rbm}} = \sum_{i=0}^n \alpha_j\beta_{i,j} \; ,
\end{equation}

where $j$ again is the corresponding state $\ket{b} = \ket{\psi_j}$. The local energy of each sample can then be calculated by this code snippet.

\begin{minted}{python}
def local_energy(H, amplitudes):
    weight, non_zero_mask, mask = amplitudes
    non_zero = weight[non_zero_mask]
    non_zero_H = H[non_zero_mask]
    
    E = torch.sum(non_zero[:,None]*non_zero_H, dim=0)/weight

    return E[mask]
\end{minted}

Where we have removed the rows of the Hamiltonian matrix whose corresponding basis has a amplitude of zero in our machine state. The 5th line is where the contributions are summed together, where we use a broadcasting functionality of PyTorch.

\begin{minted}{python}
    E = torch.sum(non_zero[:,None]*non_zero_H, dim=0)/weight
\end{minted}

If we imagine a basis size of four states we would have a \mintinline{python}{weight} array in the form of:

$$\text{\mintinline{python}{weight}} = \begin{bmatrix} \sqrt{\frac{N_0}{m}} & \sqrt{\frac{N_1}{m}} &  \sqrt{\frac{N_2}{m}} &  \sqrt{\frac{N_3}{m}}\end{bmatrix} \; .$$

Assuming that each of the approximated coefficients are greater than zero to make visualization easier, we have that $\mintinline{python}{non_zero}=\mintinline{python}{weight}$. The \mintinline{python}{[:,None]} is to transpose the \mintinline{python}{non_zero} array:

$$\text{\mintinline{python}{non_zero[:, None]}} = \begin{bmatrix}\sqrt{\frac{N_0}{m}} \\ \sqrt{\frac{N_1}{m}} \\  \sqrt{\frac{N_2}{m}} \\  \sqrt{\frac{N_3}{m}}\end{bmatrix} \; .$$

The \mintinline{python}{*} symbol is for element-wise multiplication. The Hamiltonian matrix is in python a two-dimensional array where it is represented as a array of rows. The multiplication as it stands now doesn't match:

$$\text{\mintinline{python}{non_zero[:, None]*non_zero_H}} = \begin{bmatrix}\sqrt{\frac{N_0}{m}} \\ 
\sqrt{\frac{N_1}{m}} \\  \sqrt{\frac{N_2}{m}} \\  \sqrt{\frac{N_3}{m}}\end{bmatrix}* 
\begin{bmatrix}
\beta_{0,0} & \beta_{0,1} & \beta_{0,2} &\beta_{0,n} \\
\beta_{1,0} & \beta_{1,1} & \beta_{1,2} &\beta_{1, n} \\
\beta_{2,0} & \beta_{2,1} & \beta_{2,2}& \beta_{2, 2} \\
\beta_{3,0} & \beta_{3,1} & \beta_{3,2}& \beta_{3, 3}\end{bmatrix}\; .$$


The \mintinline{python}{non_zero} is therefore expanded in the direction it the axis of too few elements:

$$\text{\mintinline{python}{non_zero[:, None]*non_zero_H}} = \begin{bmatrix}\sqrt{\frac{N_0}{m}} \\ 
\sqrt{\frac{N_1}{m}} \\  \sqrt{\frac{N_2}{m}} \\  \sqrt{\frac{N_3}{m}}\end{bmatrix} \rightarrow \begin{bmatrix}\sqrt{\frac{N_0}{m}} & \sqrt{\frac{N_0}{m}} &  \sqrt{\frac{N_0}{m}} &\sqrt{\frac{N_0}{m}} \\
\sqrt{\frac{N_1}{m}} & \sqrt{\frac{N_1}{m}} & \sqrt{\frac{N_1}{m}}& \sqrt{\frac{N_1}{m}} \\
\sqrt{\frac{N_2}{m}} & \sqrt{\frac{N_2}{m}} & \sqrt{\frac{N_2}{m}}& \sqrt{\frac{N_2}{m}} \\
\sqrt{\frac{N_3}{m}} & \sqrt{\frac{N_3}{m}} & \sqrt{\frac{N_3}{m}}& \sqrt{\frac{N_3}{m}}\end{bmatrix}\; .$$

So we get that

$$\text{\mintinline{python}{non_zero[:, None]*non_zero_H}} = \begin{bmatrix}\sqrt{\frac{N_0}{m}}\beta_{0,0} & \sqrt{\frac{N_0}{m}}\beta_{0,1} &  \sqrt{\frac{N_0}{m}}\beta_{0,2} &\sqrt{\frac{N_0}{m}}\beta_{0,n} \\
  \sqrt{\frac{N_1}{m}}\beta_{1,0}&\sqrt{\frac{N_1}{m}}\beta_{1,1} & \sqrt{\frac{N_1}{m}}\beta_{1,2} &\sqrt{\frac{N_1}{m}}\beta_{1, n} \\
\sqrt{\frac{N_2}{m}}\beta_{2,0} & \sqrt{\frac{N_2}{m}}\beta_{2,1} & \sqrt{\frac{N_2}{m}}\beta_{2,2}& \sqrt{\frac{N_2}{m}}\beta_{2, 2} \\
\sqrt{\frac{N_3}{m}}\beta_{3,0} & \sqrt{\frac{N_3}{m}}\beta_{3,1} & \sqrt{\frac{N_3}{m}}\beta_{3,2}& \sqrt{\frac{N_3}{m}}\beta_{3, 3}\end{bmatrix}\; . $$

We then sum the rows together and divide by the machine state amplitudes. The calculation is only done on the basis states and then we afterwards extract the corresponding energy for each sample through as mask \mintinline{python}{E[mask]}. To do this calculation if the first place we need to derive the approximated amplitudes. To do so we first we create an array of basis states.

\begin{minted}{python}
def create_basis(n, dtype, device):
    basis = torch.tensor(
        list(itertools.product([0, 1], repeat=n)),
        dtype = dtype,
        device= device
    )
    return basis
\end{minted}

where the \mintinline{python}{itertools.product(...)} creates all combinations of 0 and 1 into a array, and the rest is to make into a PyTorch tensor. With the basis in place we can check it against our samples. Then we check each sample which basis state it is:

\begin{minted}{python}
def amplitudes(samples, basis):
    size = samples.shape[0]
    weight = torch.sum(torch.all(samples[:, None] == basis, dim = -1), dim=0)
    weight = weight/size
    weight = torch.sqrt(weight)
    non_zero_mask = torch.where(weight!=0)
    weight[weight==0] = np.sqrt(1/size)
    mask = torch.where(torch.all(samples[:, None] == basis, dim = -1))[1]

    return weight, non_zero_mask, mask
\end{minted}

Where we once again use the broadcasting functionality to check each sample against each basis state \mintinline{python}{samples[:, None] == basis}, and sum the resulting matches. We then divide by the number of samples $m$ and take the square root as dictated by $\sqrt{\frac{N_i}{m}}$ approximation of the amplitudes. We then change the amplitudes to the basis states that isn't represented in the sample set as if it had one anyway, because we need to avoid division by zero in the \mintinline{python}{local_energy(H, samples)} function. The local energy can then be calculated if we have the Hamiltonian matrix, but the construction of these are unique to each model of course.

\subsection{The Lipkin Model}


The Lipkin model hamiltonian is defined as

\begin{equation} 
    H = \sum_{p\sigma} \left ( \frac{1}{2} \sigma \varepsilon \right ) \hc{a}{p\sigma}\op{a}{p\sigma} + \frac{V}{2}\sum_{pp'\sigma} \hc{a}{p\sigma} \hc{a}{p'\sigma} \op{a}{p'-\sigma}\op{a}{p-\sigma} + \frac{W}{2} \sum_{pp'\sigma} \hc{a}{p\sigma} \hc{a}{p'-\sigma} \op{a}{p'\sigma}\op{a}{p-\sigma} \; , 
\end{equation}

and we have rewritten it in terms of the quasi spin operators:

$$H = \varepsilon \op{J}{z} + \frac{V}{2} \left ( \op{J}{+}\op{J}{+}  + \op{J}{-}\op{J}{-} \right ) + \frac{W}{2} \left ( -N + \op{J}{+} \op{J}{-} + \op{J}{-}\op{J}{+} \right )$$

We use this to construct our Hamiltonian matrix. First we construct the quasi-spin matrices with that for $\ket{S, m}$ $m =\in {-S, \dots, S}$:

\begin{align*}
  \bra{m'} \hat{S}_z \ket{m} &= \delta_{m',m}m \\
  \bra{m'} \hat{S}_+ \ket{m} &= \delta_{m', m+1}\sqrt{S(S+1)-m'm} \\
  \bra{m'} \hate{S}_- \ket{m} &= \delta_{m'+1, m}\sqrt{S(S+1)-m'm} \\
\end{align*}

Done by the following code snippet

\begin{minted}{python}
def construct_spin(S):

  size = int(2*S + 1)
  J_pluss = np.zeros((size, size))
  J_minus = np.zeros((size, size))
  J_z = np.zeros((size, size))
  J_y = np.zeros((size, size))
  J_x = np.zeros((size, size))

  for i in range(size):
      for k in range(size):
          m = i-S
          n = k-S

          pm_factor = np.sqrt(S*(S+1) -m*n)

          J_pluss[i, k] = dd(m, n+1)*pm_factor
          J_minus[i, k] = dd(m+1, n)*pm_factor
          J_z[i, k] = dd(m,n)*m
          J_x[i, k] = (dd(m, n+1)+dd(m+1,n))*pm_factor
          J_y[i, k] = (dd(m, n+1)-dd(m+1,n))*pm_factor

  
  return J_z, J_pluss, J_minus, 0.5*J_x, (0.5+0.5j)*J_y
\end{minted}

We then insert the correct spin to get the size that we need and multiply the matrices together according to the Hamiltonian:

\begin{minted}{python}
def lipkin_hamiltonian(n, eps, V, W):
    J_z, J_pluss, J_minus = construct_spin(n/2)[:3]
    N = np.eye(int(n/2)+1)*n
    H = eps*J_z 
    H += V/2*(J_pluss@J_pluss + J_minus@J_minus)
    H += W/2*(-N + J_pluss@J_minus+J_minus@J_pluss)
    return H
\end{minted}

Where \mintinline{python}{N = np.eye(int(n/2)+1)*n} is the number operator.

\subsection{The Ising Model}

The background for the construction of the Ising model Hamiltonian matrix has been explained in the theory section \ref{sec:ising_theory}, but we will repeat some of it here. The Hamiltonian is defined as

\begin{equation}
  H(\boldsymbol{\sigma}) =J\sum_{\langle i,j\rangle} \sigma^z_i\sigma^z_j + L \sum_j \sigma^x_j \; ,
\end{equation}

And we interpret the Pauli matrices as acting on the tensor product space of our system:

\begin{equation}
  \sigma_i^x = \mathbb{I}_{2, 1} \otimes \dots \otimes \sigma_{x, i} \otimes \dots \mathbb{I}_{2, N} \; .
\end{equation}
\begin{equation}
  \sigma_i^z \sigma_{i+1}^z = \mathbb{I}_{2, 1} \otimes \dots \otimes \sigma_{z, i} \otimes \sigma_{z, i+1} \otimes \dots \otimes \mathbb{I}_{2, N} \; .
\end{equation}

For two dimensions we need to think of coupling between rows as well as shown in the rewritten Hamiltonian:

\begin{equation}
  H_{2D}(\boldsymbol{\sigma}) =J\sum_{i = 1}^M\sum_{j = 1}^N \left (\sigma^z_{i, j}\sigma^z_{i, j + 1} + \sigma_{i, j}^z\sigma^z_{i+1, j} \right )+ L \sum_{i = 1}^M\sum_{j = 1}^N\sigma^z_{i, j} \; .
\end{equation}

To construct the Hamiltonian we will use a python library called NetKet \cite{netket3:2022}\cite{netket2:2019}\cite{mpi4jax:2021}, which is a library for study of many-body quantum systems with machine learning. We use their construction example from their website \cite{netketising}.

\begin{minted}{python}
# From https://netket.readthedocs.io/en/v3.11.4/tutorials/gs-ising.html
def ising_hamiltonian(N, M, J, L):
    import netket.hilbert as hb
    from netket.operator.spin import sigmax,sigmaz
        hi = hb.Spin(s=1 / 2, N=N*M)
    H = sum([L*sigmax(hi,i) for i in range(N)])
    if M == 1:
        H += sum([J*sigmaz(hi,i)*sigmaz(hi,(i+1)%N) for i in range(N)])
    else:
        for _ in range(M):
            for i in range(N):
                H += J*sigmaz(hi,N*i)*sigmaz(hi,(N*i+1)%N)
                H += J*sigmaz(hi,N*i)*sigmaz(hi, (N*i+N)%(N*M)) 
    return np.array(H.to_dense())
\end{minted}

Where we have added the part for the second dimension.

\subsection{The Heisenberg model}

The Heisenberg model's difference to the Ising model is the input, we go from binary spin to continuous, so we reuse the Ising model hamiltonians for both one and two dimensions. To change to continuous spin we use the \mintinline{python}{given_v} instead of the \mintinline{python}{binary} from the layer sampling functions described in the section about the solver. 

\subsection{The Pairing model} \label{sec:pairing_imp}

The Pairing model Hamiltonian is given by

\begin{equation}
  \hat{H} = \sum_{\alpha\beta} \bra{\alpha}h_0\ket{\beta}\hc{a}{\alpha}\op{a}{\beta} + \frac{1}{4} \sum_{\alpha\beta\gamma\delta} \bra{\alpha\beta}\hat{V}\ket{\gamma\delta} \hc{a}{\alpha}\hc{a}{\beta}\op{a}{\delta}\op{a}{\gamma}
\end{equation}





