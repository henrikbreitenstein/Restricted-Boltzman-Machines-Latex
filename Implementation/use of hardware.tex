\section{CUDA and torch}

When we have large amounts of data and do the same unconditional operations on them, we can reduce the time it takes for the RBM to find the ground state by parallelizing parts of the calculations. The GPU, graphical processing unit, is designed for such tasks. To use the GPU we need a interface that can communicate with it. There are several options for GPU interfaces, but for NVIDIA\cite{NVIDIA} GPU's there is the CUDA\cite{cuda} interface. For simplicity we will use a python library: PyTorch\cite{paszke2019pytorch}. PyTorch is a machine learning library which has CUDA integration possibilities. To enable the use of the GPU we first initialize the GPU as a device:

\begin{minted}{python}
import torch

device = torch.device('cuda')
\end{minted}

Then when we create a tensor we do so with the GPU as its device:

\begin{minted}{python}
tensor = torch.tensor(elements, dtype=torch.float64, device=device)
\end{minted}

where \mintinline{python}{elements} is the whole object we want on the GPU. Operations on \mintinline{python}{tensor} will then be run on the processors in the GPU if possible.
